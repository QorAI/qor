# QOR â€” The Qore Mind

**A self-aware AI that knows what it knows, knows what it doesn't, and never guesses.**

QOR is not just a language model â€” it's a complete AI system with multi-speed memory,
self-modifying neurons, confidence-gated responses, live tool calling, and
zero-hallucination protection. Built on the Nested Learning paradigm
(Google Research, NeurIPS 2025).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        QOR ARCHITECTURE                              â”‚
â”‚                                                                      â”‚
â”‚  User asks a question                                                â”‚
â”‚       â”‚                                                              â”‚
â”‚       â–¼                                                              â”‚
â”‚  â”Œâ”€ CONFIDENCE GATE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  "Do I know this?" â†’ measure surprise level                â”‚     â”‚
â”‚  â”‚  "Is this live data?" â†’ price/weather/news/sports?         â”‚     â”‚
â”‚  â”‚  "Is my data fresh?" â†’ check timestamps                    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚          HIGH conf      LOW conf       LIVE DATA                     â”‚
â”‚               â”‚              â”‚              â”‚                        â”‚
â”‚               â–¼              â–¼              â–¼                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚   Answer     â”‚  â”‚   Search     â”‚  â”‚  Call Tool/   â”‚              â”‚
â”‚  â”‚   from       â”‚  â”‚   Knowledge  â”‚  â”‚  API for      â”‚              â”‚
â”‚  â”‚   Memory     â”‚  â”‚   Base (RAG) â”‚  â”‚  Fresh Data   â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                           â”‚                 â”‚                        â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                    â–¼                                 â”‚
â”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚                           â”‚   UPDATE     â”‚                           â”‚
â”‚                           â”‚   MEMORY     â”‚ â† learns for next time   â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                    â”‚                                 â”‚
â”‚                                    â–¼                                 â”‚
â”‚                              Answer User                             â”‚
â”‚                         (with source + confidence)                   â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€ QOR MODEL (inside) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Attention (RoPE) â†’ what to focus on                      â”‚      â”‚
â”‚  â”‚  Self-Modifying Neurons â†’ adapt in real-time              â”‚      â”‚
â”‚  â”‚  Multi-Speed Memory (CMS):                                â”‚      â”‚
â”‚  â”‚     âš¡ Fast   = working thoughts  (updates every step)    â”‚      â”‚
â”‚  â”‚     â—† Medium = recent knowledge  (updates every 16 steps) â”‚      â”‚
â”‚  â”‚     ğŸ§  Slow   = deep knowledge   (updates every 64 steps) â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Table of Contents

1. [Why QOR Is Different](#why-qor-is-different)
2. [Installation](#installation)
3. [Quick Start (5 Minutes)](#quick-start)
4. [Step-by-Step Guide](#step-by-step-guide)
5. [Zero Hallucination System](#zero-hallucination-system)
6. [Adding Tools & APIs](#adding-tools--apis)
7. [Knowledge Sources](#knowledge-sources)
8. [Training Data Sources](#training-data-sources)
9. [All Commands](#all-commands)
10. [API Server](#api-server)
11. [Deployment](#deployment)
12. [Project Structure](#project-structure)
13. [Model Sizes](#model-sizes)
14. [Technical Architecture](#technical-architecture)
15. [Troubleshooting](#troubleshooting)

---

## Why QOR Is Different

| Problem | How Others Solve It | How QOR Solves It |
|---------|--------------------|--------------------|
| Model doesn't know something | Guesses (hallucination!) | Detects low confidence â†’ looks it up â†’ says "I don't know" if nothing found |
| Someone asks for live data | Returns stale training data | Detects live data topic â†’ calls API â†’ returns real data â†’ updates memory |
| Model forgets old knowledge | Doesn't learn at all (frozen) | Multi-speed CMS: fast memory learns new, slow memory protects old |
| RAG searches every single time | Yes, every query = search | Learns from lookups â†’ next time answers from memory (no search needed) |
| Knowledge gets outdated | Retrain the whole model ($$$) | Continual learning: drop new files in folder â†’ model absorbs |

---

## Installation

### Requirements

- **Python**: 3.9 or higher
- **Operating System**: Windows, macOS, or Linux
- **GPU** (optional): NVIDIA GPU with CUDA for faster training (CPU works fine too)
- **RAM**: 4GB minimum, 8GB recommended
- **Disk**: 500MB for the project + space for training data

### Step 1: Install Python

If you don't have Python, download from https://python.org

Check your version:
```bash
python --version    # Should show 3.9 or higher
```

### Step 2: Install PyTorch

**With NVIDIA GPU (faster training):**
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

**CPU only (works fine for small/medium models):**
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
```

**macOS (Apple Silicon):**
```bash
pip install torch torchvision torchaudio
```

### Step 3: Install Dependencies

**Required (minimum):**
```bash
pip install tokenizers flask flask-cors
```

**Recommended (full features):**
```bash
pip install tokenizers flask flask-cors duckduckgo-search
```

**All features:**
```bash
pip install -r requirements.txt
```

Or install everything at once:
```bash
pip install torch tokenizers flask flask-cors duckduckgo-search safetensors gradio huggingface_hub onnx
```

### Step 4: Download QOR

Extract the `qor.zip` file to any folder:

```
my-project/
â”œâ”€â”€ qor/          â† The QOR package (extracted from zip)
â”œâ”€â”€ data/         â† Training data goes here (created by setup)
â”œâ”€â”€ learn/        â† Drop new files here for continual learning
â”œâ”€â”€ knowledge/    â† Documents for RAG knowledge base
â””â”€â”€ checkpoints/  â† Trained models saved here
```

### Step 5: Verify Installation

```bash
cd my-project
python -m qor setup
```

You should see:
```
QOR Setup Complete!
  Created: data/
  Created: learn/
  Created: knowledge/
  Created: checkpoints/
  Generated sample training data
```

If you see errors, check the [Troubleshooting](#troubleshooting) section.

---

## Quick Start

### The 5-Minute Version

```bash
# 1. Set up project folders and sample data
python -m qor setup

# 2. Train the tokenizer (learns vocabulary from your data)
python -m qor tokenizer

# 3. Train the model (5-15 minutes on GPU, 20-40 min on CPU)
python -m qor train --device cuda      # GPU
python -m qor train --device cpu       # CPU (also works)

# 4. Chat with your model
python -m qor chat
```

That's it â€” you have a working AI model.

### The 15-Minute Version (with real data)

```bash
# 1. Set up
python -m qor setup

# 2. Download real training data
python prepare_data.py --source wikipedia      # General knowledge
python prepare_data.py --source tinystories    # Story understanding

# 3. Train (with more steps for real data)
python -m qor tokenizer
python -m qor train --device cuda --steps 20000

# 4. Add documents to knowledge base for RAG
# Put your .txt files in knowledge/ folder

# 5. Chat with zero-hallucination protection
python -m qor chat
```

### The Production Version

```bash
# 1. Prepare data
python prepare_data.py --source all

# 2. Train medium model (better quality)
python -m qor tokenizer
python -m qor train --device cuda --size medium --steps 50000

# 3. Add knowledge base documents
# Copy your company docs, manuals, FAQs to knowledge/

# 4. Start zero-hallucination API server
python -m qor serve

# 5. Query the API
curl -X POST http://localhost:8000/think \
  -H "Content-Type: application/json" \
  -d '{"query": "What is the price of Bitcoin?"}'
```

---

## Step-by-Step Guide

### Step 1: Get Training Data

Training data is any text you want QOR to learn from. The more relevant data, the better.

**Option A: Use sample data (quickest, for testing)**
```bash
python -m qor setup
# Creates sample training data automatically
```

**Option B: Download free datasets (best for general knowledge)**
```bash
python prepare_data.py --source wikipedia       # Wikipedia articles
python prepare_data.py --source tinystories     # Short stories
python prepare_data.py --source openwebtext     # Web articles
python prepare_data.py --source all             # All of the above
```

**Option C: Use your own data (best for domain-specific)**
```bash
# Create my_data/ folder and put your .txt files there:
mkdir my_data
# Copy your files: company docs, manuals, articles, books, etc.
# Then:
python prepare_data.py --source custom
```

**Check your data:**
```bash
python prepare_data.py --source stats
```

Shows:
```
  File                           Size      Words
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  sample_knowledge.txt          150,000      25,000
  wikipedia.txt               5,000,000     850,000
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  TOTAL                       5,150,000     875,000

  Estimated training quality:
  â—†  Medium â€” good for a focused domain model
```

### Step 2: Train Tokenizer

The tokenizer learns the vocabulary from YOUR data:
```bash
python -m qor tokenizer
```

This creates `tokenizer.json` â€” the vocabulary QOR will use.

### Step 3: Train the Model

```bash
# Small model â€” fast, good for testing (5-15 min)
python -m qor train --device cuda

# Medium model â€” better quality (30-60 min)
python -m qor train --device cuda --size medium

# Large model â€” best quality (2-4 hours)
python -m qor train --device cuda --size large

# CPU training (slower but works)
python -m qor train --device cpu

# Custom training length
python -m qor train --device cuda --steps 50000

# Resume interrupted training
python -m qor train --device cuda --resume
```

You'll see output like:
```
  Step 1000 | Loss: 4.231 | LR: 0.000300 | Tok/s: 12,500
  Step 2000 | Loss: 3.187 | LR: 0.000285 | Tok/s: 12,800
  Step 3000 | Loss: 2.456 | LR: 0.000260 | Tok/s: 13,100
  ...
  Checkpoint saved: checkpoints/best_model.pt
```

### Step 4: Test Your Model

```bash
# Chat interactively
python -m qor chat

# Generate from a prompt
python -m qor generate "The future of AI is"

# Run the Mind Test (tests continual learning)
python -m qor test

# Full evaluation (perplexity, generation quality)
python -m qor eval
```

### Step 5: Set Up Knowledge Base (RAG)

Put documents in the `knowledge/` folder that you want QOR to search:
```
knowledge/
â”œâ”€â”€ company_policy.txt
â”œâ”€â”€ product_manual.txt
â”œâ”€â”€ faq.txt
â”œâ”€â”€ pricing.txt
â””â”€â”€ any_other_documents.txt
```

QOR will search these when it's not confident about an answer.

### Step 6: Start the Server

```bash
# Simple API
python -m qor serve

# Production API with zero-hallucination
python -m qor serve --port 8000
```

### Step 7: Add Continual Learning

Drop new files in `learn/` folder anytime:
```bash
# Learn from new files once
python -m qor learn

# Auto-learn whenever new files appear
python -m qor watch
```

QOR absorbs new knowledge WITHOUT forgetting old knowledge.

---

## Zero Hallucination System

This is what makes QOR different from every other AI model.

### The Problem With Normal AI

```
Normal AI:
  User: "What is Bitcoin price today?"
  AI: "Bitcoin is $45,230" â† MADE UP. Could be totally wrong.
  
  User: "What is our return policy?"
  AI: "Returns are accepted within 30 days" â† GUESSED. Never read the policy.

  No way to know if the answer is real or invented.
```

### How QOR Solves It

```
QOR:
  User: "What is Bitcoin price today?"
  
  QOR thinks:
    1. "price" + "bitcoin" â†’ this is LIVE DATA (changes every second)
    2. Check memory â†’ last BTC data was 3 hours ago â†’ STALE
    3. Call price API â†’ CoinGecko returns $97,432.15
    4. Answer with REAL data
    5. Save to memory (so next query in 5 min = instant answer)
    
  QOR: "Bitcoin is currently $97,432.15 USD (+2.3% today)"
  Source: tool:price_lookup | Confidence: HIGH âœ…
```

```
QOR:
  User: "What is our return policy?"
  
  QOR thinks:
    1. Not live data (policy doesn't change every second)
    2. Measure confidence â†’ LOW (never learned return policies)
    3. Search knowledge base â†’ Found "company_policy.txt"
    4. Read: "Returns accepted within 60 days of purchase"
    5. Answer from the ACTUAL document
    6. Save to memory (next time = instant answer)
    
  QOR: "Returns are accepted within 60 days of purchase."
  Source: knowledge_base | Confidence: MEDIUM âœ…
```

```
QOR:
  User: "What is the airspeed velocity of a dragon?"
  
  QOR thinks:
    1. Not live data
    2. Measure confidence â†’ VERY LOW
    3. Search knowledge base â†’ nothing found
    4. Search web â†’ nothing reliable
    5. Be HONEST
    
  QOR: "I don't have reliable information about this. I'd rather
        be honest than guess and give you wrong information."
  Source: unknown | Confidence: LOW âœ… (no hallucination!)
```

### Live Data Detection

QOR automatically detects when a question is about changing data:

| Category | Trigger Keywords | Auto-Refresh After |
|----------|-----------------|-------------------|
| **Prices** | price, cost, worth, BTC, stock, market | 5 minutes |
| **Weather** | weather, temperature, forecast, rain | 30 minutes |
| **News** | latest, breaking, today, current, recent | 60 minutes |
| **Sports** | score, game, match, standings | 15 minutes |
| **Availability** | in stock, open, closed, hours | 10 minutes |
| **Exchange rates** | exchange rate, convert, currency | 15 minutes |
| **Traffic** | traffic, route, delays | 5 minutes |

Static facts (capital of France, who invented the lightbulb) are trusted from memory forever.

### Confidence Levels

| Confidence | What QOR Does | Example |
|-----------|--------------|---------|
| HIGH (>85%) | Answers from internal knowledge | "What color is the sky?" |
| MEDIUM (60-85%) | Answers but checks knowledge base too | "What year was Python created?" |
| LOW (30-60%) | Searches knowledge base first | "What's our company's vacation policy?" |
| VERY LOW (<30%) | Says "I don't know" honestly | "What did John say in yesterday's meeting?" |

---

## Adding Tools & APIs

QOR can call any external service for real-time data. This is how you connect it
to your world.

### Built-In Tools

| Tool | What It Does | Needs |
|------|-------------|-------|
| Price lookup | Crypto prices (BTC, ETH, etc.) | Internet (CoinGecko, free) |
| Date/Time | Current date, time, day | Nothing |
| Web search | Search the web | `pip install duckduckgo-search` |
| Calculator | Math calculations | Nothing |
| File reader | Read local files | Nothing |

### Adding Your Own Tools

It takes 3 lines to add any API:

```python
from qor.confidence import ConfidenceGate

gate = ConfidenceGate(model, tokenizer)

# Example: Add a weather API
def weather_api(query: str) -> str:
    import requests
    API_KEY = "your-free-key-from-openweathermap.org"
    city = "London"  # extract from query
    url = f"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={API_KEY}&units=metric"
    data = requests.get(url).json()
    return f"Weather in {city}: {data['weather'][0]['description']}, {data['main']['temp']}Â°C"

gate.tools.register("weather", "Get weather data", weather_api, ["weather"])
```

Now when someone asks "What's the weather in London?", QOR automatically:
1. Detects "weather" keyword â†’ live data
2. Calls your weather API
3. Returns real weather data
4. Saves to memory (next query within 30 min = instant answer)

### More Tool Examples

**Company database:**
```python
def company_db(query: str) -> str:
    import sqlite3
    conn = sqlite3.connect("company.db")
    results = conn.execute("SELECT * FROM products WHERE name LIKE ?",
                           (f"%{query}%",)).fetchall()
    return str(results)

gate.tools.register("company_db", "Search product database", company_db,
                     ["product", "inventory", "price"])
```

**Stock prices:**
```python
def stock_price(query: str) -> str:
    import yfinance as yf
    ticker = yf.Ticker("AAPL")  # extract from query
    return f"Apple: ${ticker.info['currentPrice']}"

gate.tools.register("stocks", "Get stock prices", stock_price,
                     ["stock", "share price", "market"])
```

**Any REST API:**
```python
def my_api(query: str) -> str:
    import requests
    r = requests.get(f"https://api.example.com/search?q={query}")
    return r.json()["result"]

gate.tools.register("my_api", "Search my service", my_api, ["topic1", "topic2"])
```

See `custom_tools.py` for more ready-to-use templates.

---

## Knowledge Sources

QOR uses 3 layers of knowledge, each for a different purpose:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚  LAYER 1: TRAINING DATA (deep, permanent knowledge)         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚  How: Put .txt files in data/ â†’ train the model             â”‚
â”‚  What: Knowledge becomes part of the model's "brain"        â”‚
â”‚  Best for: General knowledge, language ability,             â”‚
â”‚            domain expertise                                  â”‚
â”‚  Example: Medical textbooks, legal docs, manuals            â”‚
â”‚  Like: A doctor's years of medical school                   â”‚
â”‚                                                              â”‚
â”‚  LAYER 2: CONTINUAL LEARNING (add knowledge anytime)        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚  How: Drop .txt files in learn/ â†’ run `python -m qor learn` â”‚
â”‚  What: Model absorbs WITHOUT forgetting old knowledge       â”‚
â”‚  Best for: New documents, policy changes, updates           â”‚
â”‚  Example: New research papers, updated procedures           â”‚
â”‚  Like: A doctor reading the latest medical journals         â”‚
â”‚                                                              â”‚
â”‚  LAYER 3: RAG + TOOLS (instant lookup, always current)      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚  How: Put docs in knowledge/ + register API tools           â”‚
â”‚  What: QOR searches when unsure, calls APIs for live data   â”‚
â”‚  Best for: Large doc collections, real-time data            â”‚
â”‚  Example: Product catalog, price feeds, weather data        â”‚
â”‚  Like: A doctor checking a drug reference book              â”‚
â”‚                                                              â”‚
â”‚  All 3 layers work TOGETHER:                                â”‚
â”‚    Check internal knowledge (fast) â†’                        â”‚
â”‚    Search knowledge base if unsure (medium) â†’               â”‚
â”‚    Call API if live data needed (accurate) â†’                â”‚
â”‚    UPDATE MEMORY after every lookup (learn for next time)   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Training Data Sources

### Free Datasets

| Source | Size | Best For | Command |
|--------|------|----------|---------|
| Sample data | 500KB | Quick testing | `python prepare_data.py --source sample` |
| Wikipedia | ~19GB | General knowledge | `python prepare_data.py --source wikipedia` |
| TinyStories | ~1GB | Small model training | `python prepare_data.py --source tinystories` |
| OpenWebText | ~40GB | Web knowledge | `python prepare_data.py --source openwebtext` |
| Your own files | Any | Your domain | `python prepare_data.py --source custom` |
| Everything | â€” | Maximum knowledge | `python prepare_data.py --source all` |

### Using Your Own Data

Put any `.txt` files in `my_data/` folder:
- Company documents, manuals, FAQs
- Books, articles, blog posts
- Chat logs, emails, notes
- Code, documentation
- Any text you want QOR to learn

```bash
mkdir my_data
# Copy your text files into my_data/
python prepare_data.py --source custom
python -m qor tokenizer
python -m qor train --device cuda
```

### How Much Data Do You Need?

| Data Size | Model Quality | Use Case |
|-----------|--------------|----------|
| <100KB | Basic patterns only | Quick testing |
| 100KB - 1MB | Simple responses | Personal assistant |
| 1MB - 10MB | Focused domain model | Company-specific bot |
| 10MB - 100MB | Good general knowledge | General assistant |
| 100MB+ | Strong understanding | Production system |

---

## All Commands

### Setup & Data

| Command | What It Does |
|---------|-------------|
| `python -m qor setup` | Create folders + sample data |
| `python prepare_data.py --source sample` | Generate sample training data |
| `python prepare_data.py --source wikipedia` | Download Wikipedia |
| `python prepare_data.py --source tinystories` | Download TinyStories |
| `python prepare_data.py --source openwebtext` | Download OpenWebText |
| `python prepare_data.py --source custom` | Use your .txt files from my_data/ |
| `python prepare_data.py --source all` | Download everything |
| `python prepare_data.py --source stats` | Show data statistics |

### Training

| Command | What It Does |
|---------|-------------|
| `python -m qor tokenizer` | Train tokenizer on your data |
| `python -m qor train` | Train small model (~5M params) |
| `python -m qor train --size medium` | Train medium (~30M params) |
| `python -m qor train --size large` | Train large (~100M params) |
| `python -m qor train --device cuda` | Train on GPU |
| `python -m qor train --device cpu` | Train on CPU |
| `python -m qor train --steps 20000` | Train for specific steps |
| `python -m qor train --resume` | Resume training |

### Using the Model

| Command | What It Does |
|---------|-------------|
| `python -m qor chat` | Interactive chat |
| `python -m qor generate "prompt"` | One-shot generation |
| `python -m qor test` | Run Mind Test (continual learning) |
| `python -m qor eval` | Full evaluation |

### Serving

| Command | What It Does |
|---------|-------------|
| `python -m qor serve` | Start API server (Flask) |
| `python -m qor serve --fastapi` | Production API (FastAPI) |
| `python -m qor serve --port 9000` | Custom port |

### Continual Learning

| Command | What It Does |
|---------|-------------|
| `python -m qor learn` | Learn from files in learn/ |
| `python -m qor watch` | Auto-learn new files continuously |

---

## API Server

### Starting the Server

```bash
# Simple API
python -m qor serve

# With knowledge base for zero-hallucination
python -m qor serve --port 8000
```

### API Endpoints

| Endpoint | Method | What It Does |
|----------|--------|-------------|
| `/think` | POST | Zero-hallucination query (confidence gate) |
| `/generate` | POST | Direct text generation |
| `/stats` | GET | System statistics |
| `/health` | GET | Health check |

### Calling the API

**Zero-hallucination query (recommended):**
```bash
curl -X POST http://localhost:8000/think \
  -H "Content-Type: application/json" \
  -d '{"query": "What is the price of Bitcoin today?"}'
```

Response:
```json
{
  "answer": "Bitcoin is currently $97,432.15 USD, up 2.3% today.",
  "confidence": 0.95,
  "source": "tool:price_lookup",
  "reasoning": "Live data (price) â€” called price API",
  "learned": true
}
```

**Simple generation:**
```bash
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Tell me about cats", "temperature": 0.8}'
```

**From Python:**
```python
import requests

response = requests.post("http://localhost:8000/think", json={
    "query": "What is our return policy?",
    "max_tokens": 200,
    "temperature": 0.7,
})
result = response.json()
print(result["answer"])     # The answer
print(result["source"])     # Where it came from
print(result["confidence"]) # How confident QOR is
```

**From JavaScript:**
```javascript
const response = await fetch("http://localhost:8000/think", {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({ query: "What is BTC price?" })
});
const result = await response.json();
console.log(result.answer);
```

Works from any language â€” Python, JavaScript, Java, Swift, C#, Go, Ruby, PHP, curl.

---

## Deployment

### Where to Host

| Platform | Cost | GPU? | Best For |
|----------|------|------|----------|
| Your own PC | Free | Your GPU | Development & testing |
| Oracle Cloud | **Free** forever | CPU | Free API hosting |
| HuggingFace Spaces | Free | Free T4 GPU | Demo & sharing |
| DigitalOcean | $12/month | CPU | Small production API |
| Hetzner | â‚¬4/month | CPU | Cheapest server |
| RunPod | $0.22/hour | GPU | Production with GPU |
| Vast.ai | $0.20/hour | GPU | Cheapest GPU rental |
| AWS/GCP/Azure | Varies | Both | Enterprise |

### Docker Deployment

```dockerfile
FROM python:3.12-slim
WORKDIR /app

# Copy QOR
COPY qor/ ./qor/
COPY checkpoints/ ./checkpoints/
COPY knowledge/ ./knowledge/
COPY tokenizer.json .
COPY memory.json .

# Install dependencies
RUN pip install torch --index-url https://download.pytorch.org/whl/cpu
RUN pip install tokenizers flask flask-cors duckduckgo-search

# Start server
EXPOSE 8000
CMD ["python", "-m", "qor", "serve", "--port", "8000"]
```

Build and run:
```bash
docker build -t qor-server .
docker run -p 8000:8000 qor-server
```

### CPU vs GPU Hosting

| Model Size | GPU Speed | CPU Speed | CPU Viable? |
|-----------|-----------|-----------|-------------|
| Small (5M) | ~50 tok/s | ~15 tok/s | âœ… Yes |
| Medium (30M) | ~30 tok/s | ~5 tok/s | âœ… Acceptable |
| Large (100M) | ~15 tok/s | ~2 tok/s | âš ï¸ Slow |

For small/medium models, CPU hosting ($5-12/month) is perfectly fine.

---

## Project Structure

```
qor/
â”œâ”€â”€ README.md            â† You are here
â”œâ”€â”€ requirements.txt     â† Python dependencies
â”‚
â”œâ”€â”€ â”€â”€ CORE â”€â”€
â”œâ”€â”€ __init__.py          # Package setup
â”œâ”€â”€ __main__.py          # CLI â€” all commands
â”œâ”€â”€ config.py            # All settings (model sizes, training, serving)
â”œâ”€â”€ model.py             # QOR neural architecture
â”‚                          - RoPE positions (better generalization)
â”‚                          - RMSNorm (faster computation)
â”‚                          - Self-Modifying Neurons (adapt during inference)
â”‚                          - Continuum Memory System (3-speed memory)
â”‚                          - KV-Cache (fast generation)
â”‚
â”œâ”€â”€ â”€â”€ DATA & TRAINING â”€â”€
â”œâ”€â”€ tokenizer.py         # BPE tokenizer (train on your data)
â”œâ”€â”€ data.py              # Data loading pipeline
â”œâ”€â”€ prepare_data.py      # Download & prepare training data
â”œâ”€â”€ train.py             # Training (mixed precision, checkpoints)
â”œâ”€â”€ evaluate.py          # Evaluation (perplexity, Mind Test)
â”‚
â”œâ”€â”€ â”€â”€ ZERO HALLUCINATION â”€â”€
â”œâ”€â”€ confidence.py        # Confidence Gate â€” the brain
â”‚                          - Measures model surprise
â”‚                          - Classifies static vs live data
â”‚                          - Routes to right source
â”‚                          - Updates memory after lookup
â”œâ”€â”€ brain.py             # Full brain server (API + confidence gate)
â”œâ”€â”€ rag.py               # Knowledge base search (RAG)
â”œâ”€â”€ custom_tools.py      # Add your own APIs & tools
â”‚
â”œâ”€â”€ â”€â”€ DEPLOYMENT â”€â”€
â”œâ”€â”€ serve.py             # REST API (Flask / FastAPI)
â”œâ”€â”€ continual.py         # Continual learning pipeline
â”œâ”€â”€ export.py            # Export (ONNX, TorchScript, INT8)
â””â”€â”€ hub.py               # HuggingFace Hub + Gradio web UI

19 files total.
```

### Folders Created by QOR

```
data/           â† Training text files
learn/          â† Drop new files here for continual learning
knowledge/      â† Documents for RAG knowledge base
checkpoints/    â† Trained model weights
my_data/        â† Your custom text files (for prepare_data.py)
```

---

## Model Sizes

| Size | Parameters | VRAM | Train Time | Quality |
|------|-----------|------|-----------|---------|
| **Small** | ~5M | 2GB | 5-15 min | Basic patterns, testing, CPU-friendly |
| **Medium** | ~30M | 4GB | 30-60 min | Decent text, domain-specific tasks |
| **Large** | ~100M | 8GB | 2-4 hours | Good quality, real applications |

```bash
python -m qor train --size small   # Default, fast
python -m qor train --size medium  # Better quality
python -m qor train --size large   # Best quality
```

---

## Technical Architecture

### What's Inside the Model

**Attention**: Multi-head self-attention with RoPE (Rotary Position Embedding).
Better than absolute positions â€” the model generalizes to sequences it hasn't
seen before.

**Normalization**: RMSNorm instead of LayerNorm. Faster, fewer parameters,
same quality.

**Self-Modifying Neurons**: During inference, neurons measure the difference
between what they predicted and what actually came next. If surprised, they
temporarily adjust their own weights. Like a brain that pays extra attention
when something unexpected happens.

**Continuum Memory System (CMS)**: Three speeds of learning in one model:
- **Fast layer**: Updates every step. Working memory, current conversation.
- **Medium layer**: Updates every 16 steps. Recent knowledge, session memory.
- **Slow layer**: Updates every 64 steps. Deep knowledge, never forgotten.

This is why QOR can learn new things without forgetting old things.

**KV-Cache**: During text generation, QOR stores previously computed attention
keys and values. Only the new token needs computation. Makes generation 5-10x
faster.

**Weight Tying**: The input embedding matrix and output head share weights.
Fewer parameters, same quality.

### What's Inside the Zero-Hallucination System

**Confidence Measurement**: Entropy of the model's output distribution.
Low entropy = model is sure. High entropy = model is guessing.

**Live Data Detection**: Pattern matching on 17 keyword categories
(price, weather, news, sports, availability, exchange rates, traffic).
Configurable â€” add your own categories.

**Knowledge Freshness Tracking**: Every piece of knowledge has a timestamp.
The system knows: "I learned BTC price 3 hours ago. Price data is stale after
5 minutes. I need to refresh."

**Tool Registry**: Pluggable system for external APIs. Register any function
as a tool with keywords and categories. QOR calls the right tool automatically.

**Memory Store**: JSON-based persistent memory. Tracks what was learned, when,
from where, and how often it's been accessed.

---

## Troubleshooting

| Problem | Solution |
|---------|----------|
| `ModuleNotFoundError: No module named 'torch'` | `pip install torch` |
| `ModuleNotFoundError: No module named 'tokenizers'` | `pip install tokenizers` |
| `ModuleNotFoundError: No module named 'flask'` | `pip install flask flask-cors` |
| `No text files found in data/` | Run `python -m qor setup` first |
| `CUDA out of memory` | Use `--size small` or `--device cpu` |
| Training loss is "nan" | Reduce learning rate: edit config.py |
| Generation is gibberish | Train for more steps or use more data |
| Tool call fails | Check internet connection |
| "I don't know" too often | Add more documents to `knowledge/` folder |
| Model is slow on CPU | Use INT8 quantization (export.py) |
| Stale answers for live data | Check tool registration and API connectivity |
| `FileNotFoundError: tokenizer.json` | Run `python -m qor tokenizer` first |
| `FileNotFoundError: best_model.pt` | Run `python -m qor train` first |

### Getting Help

1. Check that Python 3.9+ is installed: `python --version`
2. Check that PyTorch is installed: `python -c "import torch; print(torch.__version__)"`
3. Check GPU availability: `python -c "import torch; print(torch.cuda.is_available())"`
4. Run setup: `python -m qor setup`
5. Check data: `python prepare_data.py --source stats`

---

## Hallucination Rates

```
Standard AI (no protection):
  Hallucination rate: ~15-30% of factual claims
  Makes up facts, numbers, dates constantly
  No concept of "I don't know"
  Stale data served as current

QOR with Confidence Gate:
  Hallucination rate: ~2-5% of factual claims
  Facts come from verified sources
  Says "I don't know" when unsure
  Live data always gets fresh lookup
  Learns from every lookup

To push even lower:
  âœ“ Use bigger model (less word garbling)
  âœ“ Better training data (fewer wrong facts)
  âœ“ Add more tools for more data types
  âœ“ Multiple source verification
  âœ“ Human review for critical answers
```

---

## License

Built on the Nested Learning paradigm. Research paper:
"Nested Learning: Adaptive Multi-Scale Learning with Unified Objectives"
(Google Research, NeurIPS 2025).

QOR (The Qore Mind) â€” provided for research and development.
